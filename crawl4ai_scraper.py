#!/usr/bin/env python3
"""
UniDataHub - Crawl4AI University Scraper
=========================================
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–æ–≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
- Crawl4AI (–±—Ä–∞—É–∑–µ—Ä–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è)
- Google Gemini 1.5 Flash (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö)
- Pydantic (–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ö–µ–º—ã)

–õ–æ–≥–∏–∫–∞:
1. –ó–∞—Ö–æ–¥–∏—Ç –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É -> –ò—â–µ—Ç —Å—Å—ã–ª–∫–∏ (About, Programs, Admissions).
2. –ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.
3. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ü–∞—Ä—Å–∏—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
4. –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –µ–¥–∏–Ω—ã–π JSON.
"""

import os
import json
import asyncio
import logging
import re
from datetime import datetime
from typing import List, Optional, Dict, Any, Set

# ---------------------------------------------------------
# –ò–ú–ü–û–†–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø
# ---------------------------------------------------------

try:
    from dotenv import load_dotenv
    load_dotenv()  # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env —Ñ–∞–π–ª–∞
except ImportError:
    pass

from pydantic import BaseModel, Field

try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig, CacheMode
    from crawl4ai.extraction_strategy import LLMExtractionStrategy
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: Crawl4AI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
    print("–í—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install crawl4ai[all]")
    exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawl4ai.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ---------------------------------------------------------
# 1. –°–•–ï–ú–ê –î–ê–ù–ù–´–• (PYDANTIC)
# ---------------------------------------------------------

class AboutUniversity(BaseModel):
    mission: Optional[str] = Field(None, description="–ú–∏—Å—Å–∏—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞.")
    history_summary: Optional[str] = Field(None, description="–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è (–≥–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω–∏—è, –≤–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã).")
    leadership: Optional[str] = Field(None, description="–ò–º—è —Ä–µ–∫—Ç–æ—Ä–∞.")
    achievements: List[str] = Field(default_factory=list, description="–°–ø–∏—Å–æ–∫ –Ω–∞–≥—Ä–∞–¥, –º–µ—Å—Ç–∞ –≤ —Ä–µ–π—Ç–∏–Ω–≥–∞—Ö (QS, THE).")

class AcademicProgram(BaseModel):
    program_name: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏/–ø—Ä–æ–≥—Ä–∞–º–º—ã.")
    degree_level: str = Field(..., description="–£—Ä–æ–≤–µ–Ω—å (–ë–∞–∫–∞–ª–∞–≤—Ä–∏–∞—Ç, –ú–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞, PhD).")
    faculty: Optional[str] = Field(None, description="–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–ª–∏ —à–∫–æ–ª–∞.")

class Admissions(BaseModel):
    requirements: Optional[str] = Field(None, description="–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è (–ï–ù–¢, –ø—Ä–æ—Ö–æ–¥–Ω—ã–µ –±–∞–ª–ª—ã).")
    deadlines: Optional[str] = Field(None, description="–î–∞—Ç—ã –ø—Ä–∏–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
    scholarships: Optional[str] = Field(None, description="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥—Ä–∞–Ω—Ç–∞—Ö –∏ —Å–∫–∏–¥–∫–∞—Ö.")
    tuition_info: Optional[str] = Field(None, description="–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å).")

class VirtualTour(BaseModel):
    is_available: bool = Field(False, description="–ï—Å—Ç—å –ª–∏ 3D —Ç—É—Ä.")
    url: Optional[str] = Field(None, description="–°—Å—ã–ª–∫–∞ –Ω–∞ 3D —Ç—É—Ä.")

class InternationalCooperation(BaseModel):
    partners: List[str] = Field(default_factory=list, description="–í–£–ó—ã-–ø–∞—Ä—Ç–Ω–µ—Ä—ã.")
    exchange_programs: Optional[str] = Field(None, description="–ü—Ä–æ–≥—Ä–∞–º–º—ã –æ–±–º–µ–Ω–∞ (Erasmus –∏ –¥—Ä).")

class UniversityStats(BaseModel):
    employment_rate: Optional[str] = Field(None, description="–ü—Ä–æ—Ü–µ–Ω—Ç —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (—Å—Ç—Ä–æ–∫–æ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä '95%').")
    student_count: Optional[str] = Field(None, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤.")

class UniversityData(BaseModel):
    """–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞."""
    university_name: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞.")
    city: Optional[str] = Field(None, description="–ì–æ—Ä–æ–¥.")
    website: Optional[str] = Field(None, description="URL —Å–∞–π—Ç–∞.")
    about: AboutUniversity = Field(default_factory=AboutUniversity)
    academic_programs: List[AcademicProgram] = Field(default_factory=list)
    admissions: Admissions = Field(default_factory=Admissions)
    virtual_tour: VirtualTour = Field(default_factory=VirtualTour)
    international: InternationalCooperation = Field(default_factory=InternationalCooperation)
    stats: UniversityStats = Field(default_factory=UniversityStats)

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
class LinkOfInterest(BaseModel):
    url: str = Field(..., description="–ü–æ–ª–Ω–∞—è —Å—Å—ã–ª–∫–∞.")
    section_type: str = Field(..., description="–¢–∏–ø: 'about', 'programs', 'admissions', 'international'.")
    title: str = Field(..., description="–¢–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏.")

class LinksDiscovery(BaseModel):
    links: List[LinkOfInterest] = Field(default_factory=list)

# ---------------------------------------------------------
# 2. –î–ê–ù–ù–´–ï (–°–ü–ò–°–û–ö –£–ù–ò–í–ï–†–°–ò–¢–ï–¢–û–í)
# ---------------------------------------------------------
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL (–±–µ–∑ Markdown —Ä–∞–∑–º–µ—Ç–∫–∏)
KAZAKHSTAN_UNIVERSITIES = [
    {'name': '–ù–∞–∑–∞—Ä–±–∞–µ–≤ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 'url': 'https://nu.edu.kz', 'city': '–ê—Å—Ç–∞–Ω–∞'},
    {'name': '–ö–∞–∑–∞—Ö—Å–∫–∏–π –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –∏–º. –∞–ª—å-–§–∞—Ä–∞–±–∏', 'url': 'https://kaznu.kz', 'city': '–ê–ª–º–∞—Ç—ã'},
    {'name': 'Satbayev University', 'url': 'https://satbayev.university', 'city': '–ê–ª–º–∞—Ç—ã'},
    {'name': '–ï–≤—Ä–∞–∑–∏–π—Å–∫–∏–π –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 'url': 'https://enu.kz', 'city': '–ê—Å—Ç–∞–Ω–∞'},
    {'name': '–ö–ë–¢–£', 'url': 'https://kbtu.kz', 'city': '–ê–ª–º–∞—Ç—ã'},
    {'name': 'KIMEP University', 'url': 'https://kimep.kz', 'city': '–ê–ª–º–∞—Ç—ã'},
    {'name': 'Astana IT University', 'url': 'https://astanait.edu.kz', 'city': '–ê—Å—Ç–∞–Ω–∞'},
    {'name': '–ú–£–ò–¢ (IITU)', 'url': 'https://iitu.edu.kz', 'city': '–ê–ª–º–∞—Ç—ã'},
    # ... —Å—é–¥–∞ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤–µ—Å—å –æ—Å—Ç–∞–ª—å–Ω–æ–π —Å–ø–∏—Å–æ–∫
]

# ---------------------------------------------------------
# 3. –ü–†–û–ú–ü–¢–´ –î–õ–Ø LLM
# ---------------------------------------------------------

EXTRACTION_INSTRUCTION = """
–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –ò–∑–≤–ª–µ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
–Ø–∑—ã–∫ –≤—ã–≤–æ–¥–∞: –†–£–°–°–ö–ò–ô.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–π –ø–æ–ª—è –ø—É—Å—Ç—ã–º–∏ (null).
–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–∞–Ω–Ω—ã–µ.

1. academic_programs: –ù–∞–π–¥–∏ 5-10 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–µ–π.
2. stats: –ò—â–∏ —Ü–∏—Ñ—Ä—ã (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤, –ø—Ä–æ—Ü–µ–Ω—Ç —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞).
3. admissions: –ò—â–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ï–ù–¢ –∏ —Å—Ä–æ–∫–∏.
"""

LINK_DISCOVERY_INSTRUCTION = """
–ù–∞–π–¥–∏ –≤ –º–µ–Ω—é —Å–∞–π—Ç–∞ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–¥–µ–ª—ã:
1. 'about' - –û –Ω–∞—Å, –ò—Å—Ç–æ—Ä–∏—è, –ú–∏—Å—Å–∏—è.
2. 'programs' - –§–∞–∫—É–ª—å—Ç–µ—Ç—ã, –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏, –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã.
3. 'admissions' - –ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ê–±–∏—Ç—É—Ä–∏–µ–Ω—Ç—É, –ü—Ä–∏–µ–º–Ω–∞—è –∫–æ–º–∏—Å—Å–∏—è.
4. 'international' - –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ, –ü–∞—Ä—Ç–Ω–µ—Ä—ã.

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ, –ø–æ–ª–Ω—ã–µ URL.
"""

# ---------------------------------------------------------
# 4. –õ–û–ì–ò–ö–ê –ü–ê–†–°–ï–†–ê
# ---------------------------------------------------------

class UniversityScraper:
    def __init__(self, api_key: str, output_file: str = "universities_data.json"):
        self.api_key = api_key
        # –Ø–í–ù–û–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï GEMINI 1.5 FLASH
        self.provider = "gemini/gemini-1.5-flash"
        self.output_file = output_file
        self.results = []
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.results = data.get('universities', [])
            except Exception:
                pass

    def _get_llm_config(self) -> LLMConfig:
        return LLMConfig(provider=self.provider, api_token=self.api_key)

    def _clean_json(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç LLM –æ—Ç Markdown –±–ª–æ–∫–æ–≤."""
        text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
        return text.strip()

    async def discover_links(self, url: str) -> List[Dict]:
        """–§–∞–∑–∞ 1: –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫."""
        strategy = LLMExtractionStrategy(
            llm_config=self._get_llm_config(),
            schema=LinksDiscovery.model_json_schema(),
            instruction=LINK_DISCOVERY_INSTRUCTION
        )
        
        config = CrawlerRunConfig(
            extraction_strategy=strategy,
            cache_mode=CacheMode.BYPASS,
            page_timeout=30000,
            wait_until="domcontentloaded" # –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ–Ω—é
        )

        async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
            try:
                result = await crawler.arun(url=url, config=config)
                if result.success and result.extracted_content:
                    clean_content = self._clean_json(result.extracted_content)
                    data = json.loads(clean_content)
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è, –µ—Å–ª–∏ –≤–µ—Ä–Ω—É–ª—Å—è —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –æ–±—ä–µ–∫—Ç
                    if isinstance(data, list) and data:
                        data = data[0]
                    return data.get('links', [])
            except Exception as e:
                logger.error(f"Link discovery failed for {url}: {e}")
        return []

    async def scrape_page(self, url: str, schema_cls=UniversityData) -> Optional[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        strategy = LLMExtractionStrategy(
            llm_config=self._get_llm_config(),
            schema=schema_cls.model_json_schema(),
            instruction=EXTRACTION_INSTRUCTION
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º magic=True –¥–ª—è –æ–±—Ö–æ–¥–∞ –∞–Ω—Ç–∏-–±–æ—Ç–æ–≤ (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤–µ—Ä—Å–∏–µ–π)
        browser_cfg = BrowserConfig(
            headless=True, 
            ignore_https_errors=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )

        async with AsyncWebCrawler(config=browser_cfg) as crawler:
            try:
                result = await crawler.arun(
                    url=url, 
                    config=CrawlerRunConfig(
                        extraction_strategy=strategy,
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=40000 # 40 —Å–µ–∫
                    )
                )
                
                if result.success and result.extracted_content:
                    clean_content = self._clean_json(result.extracted_content)
                    data = json.loads(clean_content)
                    if isinstance(data, list) and data:
                        return data[0]
                    return data
            except Exception as e:
                logger.warning(f"Failed to scrape {url}: {e}")
        return None

    def merge_data(self, main_data: Dict, new_data: Dict) -> Dict:
        """–£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü."""
        if not new_data: 
            return main_data
            
        # Helper –¥–ª—è —Å–ª–∏—è–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        def merge_lists(key, unique_key=None):
            main_list = main_data.get(key, []) or [] # Ensure list
            new_list = new_data.get(key, []) or []   # Ensure list
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, achievements)
            if not unique_key:
                main_data[key] = list(set(main_list + new_list))
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, programs), –ø—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                existing_vals = {item.get(unique_key) for item in main_list if item.get(unique_key)}
                for item in new_list:
                    if item.get(unique_key) and item.get(unique_key) not in existing_vals:
                        main_list.append(item)
                main_data[key] = main_list

        # –°–ª–∏—è–Ω–∏–µ about
        if 'about' in new_data:
            for k, v in new_data['about'].items():
                if v and not main_data.get('about', {}).get(k):
                    if 'about' not in main_data: main_data['about'] = {}
                    main_data['about'][k] = v
            # Achievements merge
            if new_data['about'].get('achievements'):
                 if 'about' not in main_data: main_data['about'] = {}
                 existing = set(main_data['about'].get('achievements', []))
                 existing.update(new_data['about']['achievements'])
                 main_data['about']['achievements'] = list(existing)

        # –°–ª–∏—è–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º (—É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ program_name)
        if 'academic_programs' in new_data:
             # –õ–æ–≥–∏–∫–∞ –≤—ã—à–µ –±—ã–ª–∞ —É–ø—Ä–æ—â–µ–Ω–∞, —Ç—É—Ç —Ä–µ–∞–ª–∏–∑—É–µ–º –≤—Ä—É—á–Ω—É—é –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
             current_progs = main_data.get('academic_programs', [])
             current_names = {p['program_name'] for p in current_progs}
             for p in new_data.get('academic_programs', []):
                 if p.get('program_name') and p['program_name'] not in current_names:
                     current_progs.append(p)
             main_data['academic_programs'] = current_progs

        # –°–ª–∏—è–Ω–∏–µ International partners
        if 'international' in new_data and new_data['international'].get('partners'):
            if 'international' not in main_data: main_data['international'] = {}
            existing = set(main_data['international'].get('partners', []))
            existing.update(new_data['international']['partners'])
            main_data['international']['partners'] = list(existing)

        return main_data

    async def process_university(self, uni_info: Dict):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞."""
        name = uni_info['name']
        url = uni_info['url']
        
        # –û—á–∏—Å—Ç–∫–∞ URL –µ—Å–ª–∏ –æ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ markdown [url](url)
        match = re.search(r'\((https?://[^)]+)\)', url)
        if match:
             url = match.group(1)
        elif url.startswith('[') and '](' in url: # [url](url) –±–µ–∑ http?
             match_alt = re.search(r'\(([^)]+)\)', url)
             if match_alt: url = match_alt.group(1)

        logger.info(f"üöÄ –°—Ç–∞—Ä—Ç: {name} ({url})")

        # 1. –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫
        links = await self.discover_links(url)
        logger.info(f"Found {len(links)} links for {name}")

        # 2. –ü–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π
        main_data = await self.scrape_page(url)
        if not main_data:
            logger.error(f"‚ùå Main page failed: {name}")
            return
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø–æ–ª—è, –µ—Å–ª–∏ AI –ø—Ä–æ–ø—É—Å—Ç–∏–ª
        main_data['university_name'] = name
        main_data['website'] = url
        main_data['city'] = uni_info['city']
        main_data['_scraped_at'] = datetime.now().isoformat()

        # 3. –ü–∞—Ä—Å–∏–Ω–≥ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–º–∞–∫—Å–∏–º—É–º 3)
        tasks = []
        processed_types = set()
        
        for link in links:
            l_type = link.get('section_type')
            l_url = link.get('url')
            
            if l_type in processed_types or not l_url.startswith('http'):
                continue
                
            processed_types.add(l_type)
            logger.info(f"  -> –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞ {l_type}: {l_url}")
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª–æ–∂–∏—Ç—å —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –±—Ä–∞—É–∑–µ—Ä
            sub_data = await self.scrape_page(l_url)
            if sub_data:
                main_data = self.merge_data(main_data, sub_data)
            
            await asyncio.sleep(1) # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å
            if len(processed_types) >= 3: break

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –µ—Å—Ç—å
        self.results = [r for r in self.results if r.get('website') != url]
        self.results.append(main_data)
        self.save_to_file()
        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {name}")

    def save_to_file(self):
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "last_update": datetime.now().isoformat(),
                "universities": self.results
            }, f, ensure_ascii=False, indent=2)

    async def run_all(self):
        for uni in KAZAKHSTAN_UNIVERSITIES:
            await self.process_university(uni)
            await asyncio.sleep(2) # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≤—É–∑–∞–º–∏

# ---------------------------------------------------------
# 5. –ó–ê–ü–£–°–ö
# ---------------------------------------------------------

async def main():
    # 1. –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á –∏–∑ ENV
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω GOOGLE_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
        print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ export GOOGLE_API_KEY='...'")
        return

    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    scraper = UniversityScraper(api_key=api_key)
    
    # 3. –ó–∞–ø—É—Å–∫
    print(f"ü§ñ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —Å –º–æ–¥–µ–ª—å—é {scraper.provider}")
    await scraper.run_all()

    # 4. –ü–æ–ø—ã—Ç–∫–∞ –æ–±–Ω–æ–≤–∏—Ç—å TypeScript —Ñ–∞–π–ª (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–∫—Ä–∏–ø—Ç)
    try:
        from generate_minimal_ts import main as gen_ts
        print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ TypeScript —Ñ–∞–π–ª–∞...")
        gen_ts()
    except ImportError:
        pass

if __name__ == "__main__":
    asyncio.run(main())
